# 实习经历

1. 构建大模型领域知识库的RAG系统，采用**LiteFlow**规则引擎编排整体检索逻辑，使用国际化分词器将语料分词，通过**Embedding算法**将FAQ和文档知识降维向量化后存入Elasticsearch，运用**向量数据库**的分批和多路结合的**N\*M阵列召回算法**，对召回结果正排后再运用**RRF混合排序算法**进行重排，构建**AC自动机**结合BitMap实现**多模字符串匹配**高亮位置返回。
2. 实现知识库文档下载服务，通过阈值判断切换单文件直连下载 or 批量文件压缩包异步处理的**双通道下载**模式，利用**Kafka**和**异步****线程池**处理高并发请求，通过**Facade模式**统一OSS文件操作，实现的单文件下载延迟TP99小于300ms，支持每秒5000+并发下载请求。
3. 参与知识库标签批量导入系统，采用**异步****线程池**实现请求快速响应和后台多线程异步处理 Excel 文件导入。结合 EasyExcel 进行解析与校验。引入Redison**分布式锁**防止并发冲突，使用 **MySQL** **状态机** 管理导入生命周期。设计 reDeal 重试机制，支持失败任务手动恢复。
4. 成功排查并解决poc环境系统周期性OOM问题，通过分析Grafana监控、rancher日志及Arthas工具，定位到分布式锁超时释放与大数据量并行处理导致的内存暴涨。

# xxljob影响定时集成流的事务管理

http://wiki.meicloud.com/pages/viewpage.action?pageId=136909016#page-metadata-start)

## 问题

bug单：http://devsecops.meicloud.com/agile/iteration/detail?sysId=7a3e13034cad402ea7866fbed8cd5898&detailId=AAEF004210&cardId=AAEF005172&cardType=4

## 结论

定时集成流的下线需要在同一个事物管理中更新集成流状态、删除xxljob中正在执行的任务，而sit环境请求xxljob删除的时间超出了mysql数据的锁等待超时时间，更新集成流状态的锁也超时，导致服务端异常。（前端10s超时早已经返回）

解决：在xxljob中针对sql增加索引

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=MGU4NzA5Y2RmOGVkNzVkNGY5ZDEzYjQwMGJhY2M3ZTJfYng2N3hzUFlTNXZPRElEa1ptRFFZdURvbm1vMDQwNUVfVG9rZW46UWJtZWJrbjFob21hZlh4NGNra2M3MVM4bnBoXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=MzM0Y2RmMWYxZjEwMjcyOWRhMDgxM2VmNzA2MzNjYmFfdnhMaTd3VWc5emJqWlpJeUpRakVzQnZKeDFmZzkxUGFfVG9rZW46QkZvdGJ4WmFpb09EaW94WHVRa2M4cjdobmloXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042056.png)

## 解决思路

### 1.xxljob优化

1. xxljob的remove接口
   1. 阻断的sql：
   2. `delete from xxl_job_log WHERE job_id = #{jobId}`
   3. ![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=OTY1OWZhN2M2MTRlMzQzMGFiOThiMmFhYzVkYzBkMjRfNWlYSGxRTmNwdkpIdDdLUW02b1I3RksxSkZ2QmpHaGZfVG9rZW46RDlvcmJyVWVlb0M1SzB4c3M4YmNzZFY1blk2XzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)
2. xxljob的stop接口

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042254.png)

### 2.ipaas优化

todolist：1.依赖第三方的请求（失败、超时）如何补偿？

## arthas排查过程分享

### 1.只打印耗时的日志

```Plaintext
trace -E org.aspectj.runtime.reflect.JoinPointImpl|org.aspectj.lang.ProceedingJoinPoint|org.aopalliance.intercept.MethodInvocation|com.meicloud.mcu.flow.model.service.impl.FlowWorkflowDefinitionServiceImpl publish|proceed ``'#cost > 20000'` `--skipJDKMethod ``false` `'1==1'` `// 日志```---ts=``2024``-``07``-``15` `18``:``24``:``56``;thread_name=http-nio-``18096``-exec-``8``;id=a9;is_daemon=``true``;priority=``5``;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader``@2c54e797``  ```---[``50974``.142491ms] org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation:proceed()``...省略``+---[``99.99``% ``50799``.472031ms ] com.meicloud.mcu.flow.model.service.impl.FlowWorkflowDefinitionServiceImpl:update() #``1160` `[``throws` `Exception]``                                ``|    |    |    |    |    |    |    |    |    |    |    |    |    `---``throw``:org.springframework.dao.CannotAcquireLockException #``264` `[``### Error updating database. Cause: com.mysql.cj.jdbc.exceptions.MySQLTransactionRollbackException: Lock wait timeout exceeded; ``try` `restarting transaction``### The error may exist in com/meicloud/mcu/flow/model/mapper/FlowWorkflowDefinitionMapper.java (best guess)``### The error may involve com.meicloud.mcu.flow.model.mapper.FlowWorkflowDefinitionMapper.update-Inline``### The error occurred ``while` `setting parameters``### SQL: UPDATE flow_workflow_definition SET status = ? WHERE is_del = ``0` `AND (id = ?)``### Cause: com.mysql.cj.jdbc.exceptions.MySQLTransactionRollbackException: Lock wait timeout exceeded; ``try` `restarting transaction``; Lock wait timeout exceeded; ``try` `restarting transaction; nested exception is com.mysql.cj.jdbc.exceptions.MySQLTransactionRollbackException: Lock wait timeout exceeded; ``try` `restarting transaction]` `trace -E org.aspectj.runtime.reflect.JoinPointImpl|org.aspectj.lang.ProceedingJoinPoint|org.aopalliance.intercept.MethodInvocation|com.meicloud.mcu.flow.model.service.impl.FlowWorkflowDefinitionServiceImpl|com.meicloud.mcu.schedule.helper.ScheduleTaskExecutor publish|proceed|removeJob ``'#cost > 20000'` `--skipJDKMethod ``false` `'1==1'` `// 日志```---ts=``2024``-``07``-``16` `16``:``52``:``54``;thread_name=http-nio-``18096``-exec-``10``;id=ac;is_daemon=``true``;priority=``5``;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader``@71a59597``  ```---[``50864``.780349ms] org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation:proceed()``...省略``+---[``99.95``% ``50763``.850281ms ] com.meicloud.job.xxl.XxlJobRestClient:removeJob() #``129` `[``throws` `Exception]``                                ``|    |    |    |    |    |    |    |    `---``throw``:com.meicloud.job.xxl.XxlJobException #``150` `[[code:``500``]--xxl-job-admin.response:org.springframework.dao.CannotAcquireLockException:``### Error updating database. Cause: com.mysql.cj.jdbc.exceptions.MySQLTransactionRollbackException: Lock wait timeout exceeded; ``try` `restarting transaction``### The error may exist in ``class` `path resource [mybatis-mapper/XxlJobLogMapper.xml]<br/>### The error may involve defaultParameterMap``### The error occurred ``while` `setting parameters<br/>### SQL: delete from xxl_job_log  WHERE job_id = ?``### Cause: com.mysql.cj.jdbc.exceptions.MySQLTransactionRollbackException: Lock wait timeout exceeded; ``try` `restarting transaction``; Lock wait timeout exceeded; ``try` `restarting transaction; nested exception is com.mysql.cj.jdbc.exceptions.MySQLTransactionRollbackException: Lock wait timeout exceeded; ``try` `restarting transaction]
```

### 2.涉及第三方的适当增加前后日志

1. 不增加日志的话，这种超时的请求无法定位到具体日志，超时不会返回traceid
2. 增加日志根据关键字查询日志，同时获取到traceid

# 东方精工IPaaS服务不断重启

http://wiki.meicloud.com/pages/viewpage.action?pageId=112918535#page-metadata-start)

## 问题

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042554.png)

## 排查过程

1. 查看grafana监控 发现2小时左右，容器内存有个爆涨，并且峰值已经超过分配给iPaaS应用的最大值了

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042844.png)

1. 这时候很自然的排查思路是设置jvm参数：-XX:+HeapDumpOnOutOfMemoryError，在java应用退出前将内存dump出来进行分析。因为这是客户生产环境，出于谨慎考虑，没有使用这种方法。

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042276.png)

1. 分析应用重启前的日志 从监控上看，每2小时左右会出现重启，于是分析每次重启前的日志，并没有太多头绪。
2. 分析定时任务 看是否存在每2小时执行的任务。这个可以查看xxl-job的控制台，结果并没有
3. 使用arthas工具，观察应用重启前的cpu和memory使用情况

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042423.png)

1. 从监控看，在重启前主要是GC线程在工作，说明的确是由于内存不足引起的。此时处于runnable状态的业务线程，比较有可能是引发内存爆掉的元凶，但从观察来看，大部分是线程池，并不能看出具体在执行什么业务。
2. 再回到日志 既然怀疑是某些任务导致的内存爆涨，那么在处理这些业务，按理应该会留下一些日志，于是，再次针对重启前的日志做详细分析。
3. 1）先找到应用重启的地方

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=ODAzNGEzMDQ1YTE0ZjI1NTYzNzQ5NGQ1MDZhN2RlMmJfbzlPQThhMXo1U1NXWUhzMlp5VU1rMUZsSE0xRHg1M0JfVG9rZW46SWt5VmI4TGZNb3k4aDV4a0lWNmNQcU15bkRkXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

1. 2）这个地方往前找，看有哪些业务在处理，并逐一分析 由于重启是有规律出现的，就有必要同时多看几个重启的日志，看是否有可疑的地方。果然发现有一处地方有重启之前出现了异常：

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=MzJjMjEwNzVlNTkyODY3NThhMmVhMDczMmUwNjVlYjNfU1YybzhuVnVsczRwMGRFSm1pQTJ0TVdacWJtZjVQbGRfVG9rZW46RHRsWGJzcWxZb2p5SDV4MzVkemNzMVhKbkxmXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

1. 3）于是针对该异常进行分析，发现它并不是定时任务，而是集成流执行1w次，批量异步地处理日志。跟随堆栈，逐个方法分析，竟然发现有个可疑点：

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042272.png)

1. 这里的逻辑是：首先获取redis分布锁，锁的超时时间设置7200s，然后再进行日志处理。这也太巧了吧，7200s不就是2小时么，既然可疑，那就再深入分析下
2. 4）看看redis的锁情况 这里就更可疑了，超时7200s，难道任务执行完了锁不释放？

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042763.png)

1. 分布式锁的代码如下，按理是应该是能正常释放的。那就可能有2种原因：1、任务执行时间过长，锁一直持有；2、获取锁后，在任务执行过程中，服务被重启了，锁被等待超时释放

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222042035.png)

1. 第1种原因比较好判断，查下任务获取锁之后，是否持续有日志输出，或者用arthas的watch或stack功能观察是否在执行。这1点很快被排除了。 第2种原因，这种的话，锁就会一直等待直至超时释放，那么就应该有日志显示，线程获取锁失败。最终验证的确如此。这样时间上也比较吻合服务2小时重启的现象。
2. 5）到现在，可以判断比较大可能是这个方法导致服务重启的。接下来，就是要找到内存会暴涨的真正原因。 获取锁之后，主要的逻辑是这个：分批聚合处理

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=MzExZTVjMzQ2NDM5ZGY1ZWExZTM5NDFjOWQwYmVkN2JfM0JxNGY5OHdZaWxESUZMVVpiUHYyMUp6bVdaQjRDZ1FfVG9rZW46WG9hZ2I0bFNTb0hlWHp4REIwR2NRMkJDbnpkXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

1. 标红这段代码逻辑引起了注意，将数据分批2w后，并行地对每个批次进行处理。如果说批次非常多的话，那么内存的占用量是非常大。于是查下数据量：

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222043042.png)

1. 其中left，right分别对应最小值和最大值，2w一批的话，总共将有5000批次。。。到此，基本确认就是这个导致的了。
2. 6）为了最终验证是这个任务导致的，将redis的锁设置为永不超时，意味着不再执行该任务。（经评估，停掉该任务不会对业务有影响，因为它只是全量兜底地日志分析统计） 果然，将redis的锁设置为永不超时后，服务6h没有再重启过了。

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222043634.png)

## 结论

1. 由于对全量数据量的评估不足，编码过程中没有考虑到并行执行可能导致内存暴掉；
2. 缺乏足够的告警机制，否则内存暴涨的问题能够尽早发现，不至于等到服务重启发现

####  为什么OOM重启时间和锁释放时间刚好相同

- 锁超时导致任务重复执行：
  - 锁的超时时间是2小时，任务执行时间接近2小时。
  - 当锁超时释放后，另一个线程可能会获取到相同的锁并开始执行相同的任务。
  - 关键点：任务重复执行进一步加剧了内存占用。
- 内存占用的累积：
  - 每次任务执行都会占用大量内存。
  - 如果任务每2小时重复执行一次，内存占用会逐渐累积。
  - 关键点：内存占用在2小时后达到峰值，导致OOM。

### 详细的时间线分析

1. 任务开始执行：
   1. 任务获取Redis锁，开始分批并行处理数据。
   2. 内存占用开始逐渐增加。
2. 锁超时释放：
   1. 2小时后，锁超时释放。
   2. 另一个线程获取到相同的锁，开始执行相同的任务。
   3. 关键点：此时内存占用已经很高，新的任务进一步加剧了内存占用。
3. 内存占用达到峰值：
   1. 新的任务启动后，内存占用迅速增加。
   2. 内存占用达到JVM分配的最大内存，导致OOM。
4. 服务重启：
   1. OOM导致服务崩溃并重启。
   2. 重启后，任务再次开始执行，循环往复。

# 下载知识文件

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222043591.png)

```Java
public List<AuthCheckDTO> downloadDocFilesById(List<String> pointIdList) {
    List<KnowDocFileDO> knowDocFileDOS = knowDocFileDomainService.selectByPointIdList(pointIdList);
    List<String> knowIdList = knowDocFileDOS.stream().map(KnowDocFileDO::getKnowId).distinct().collect(Collectors.toList());
    List<KnowDocInfoDO> knowDocInfoDOS = knowDocInfoDomainService.selectByKnowIds(knowIdList);
    return downloadDocFiles(knowDocInfoDOS, knowDocFileDOS);
}

public List<AuthCheckDTO> downloadDocFiles(List<KnowDocInfoDO> docInfoList, List<KnowDocFileDO> docFileList) {
    // 权限校验
    Map<String, KnowDocInfoDO> docInfoMap = docInfoList.stream().collect(Collectors.toMap(KnowDocInfoDO::getKnowId, Function.identity()));
    Map<String, KnowDocFileDO> docFileMap = docFileList.stream().collect(Collectors.toMap(KnowDocFileDO::getPointId, Function.identity()));

    String userId = LoginContext.getLoginInfo().getMip();
    boolean haveAuth = commApplicationService.haveTechnologyAdmin(userId);
    if (!haveAuth) {
        List<String> docIds = new ArrayList<>(docFileMap.keySet());
        DocPermissionReq req = new DocPermissionReq();
        req.setIgnoreNotExistsException(true);
        req.setDocIdList(docIds);
        req.setUserId(userId);
        List<DocPermissionRsp> docPermissionRsps = docQueryApplicationService.checkPermission(req);

        docPermissionRsps.forEach(docPermissionRsp -> {
            // 没有download时使用readable字段
            if ((docPermissionRsp.isDownload() == null && !docPermissionRsp.isReadable()) || Boolean.FALSE.equals(docPermissionRsp.isDownload())) {
                KnowDocFileDO docFileDO = docFileMap.get(docPermissionRsp.getDocId());
                KnowDocInfoDO docInfo = docInfoMap.get(docFileDO.getKnowId());
                throw new JrBizException(KnowBizErrorCode.NO_AUTH_ERROR.getOuterErrCode(), buildNoAuthMessage(docInfo, true));
            }
        });
    }
    // 如果知识点属于同一个知识，则获取知识id，否则不获取
    String knowId = docInfoList.size() == 1 ? docInfoList.get(0).getKnowId() : null;
    // 下载文档文件
    if (docFileList.size() > commonConfig.getDocFileDownloadZipThreshold()) {
        // 大于阈值的走压缩包下载
        String pointIdStr = docFileList.stream()
                // 过滤无法下载的状态
                .filter(docFile -> !DocPointDealStatusEnum.DOWNLOADING.codeEquals(docFile.getDealStatus())
                        && !DocPointDealStatusEnum.DOWNLOAD_FAIL.codeEquals(docFile.getDealStatus()))
                .map(KnowDocFileDO::getPointId)
                .sorted().collect(Collectors.joining(","));
        String pointIdMd5 = DigestUtils.md5Hex(pointIdStr);
        // 查询已有的压缩包
        KnowDocZipFileDO knowDocZipFileDO = knowDocZipFileDomainService.queryZipFile(knowId, pointIdMd5);

        if (knowDocZipFileDO == null || DocFileZipStatusEnum.ERROR.codeEquals(knowDocZipFileDO.getStatus())) {
            // 不存在或失败时重新生成压缩包，并上传到oss
            if (knowDocZipFileDO == null) {
                knowDocZipFileDO = new KnowDocZipFileDO();
            }
            knowDocZipFileDO.setKnowId(knowId);
            knowDocZipFileDO.setPointIds(pointIdStr);
            knowDocZipFileDO.setPointIdsMd5(pointIdMd5);
            knowDocZipFileDO.setStatus(1);
            knowDocZipFileDO.setCreateBy(userId);

            knowDocZipFileDomainService.saveZipFile(knowDocZipFileDO);
            try{
                // 抛出异常，返回前端固定错误码
                throw new JrBizException(KnowBizErrorCode.DOC_FILE_DOWNLOADING_ERROR);
            }finally {
                // 异步下载，并返回固定错误码
                KnowDocZipFileDO finalKnowDocZipFileDO = knowDocZipFileDO;
                taskExecutor.submit(() -> {
                    try{
                        String zipFileName = knowId == null ? IdUtil.fastSimpleUUID() + ".zip" : docInfoMap.get(knowId).getDocTitle() + ".zip";
                        String zipFileId = zipDocFiles(userId, zipFileName, docFileList);
                        finalKnowDocZipFileDO.setZipFileName(zipFileName);
                        finalKnowDocZipFileDO.setZipFileId(zipFileId);
                        finalKnowDocZipFileDO.setStatus(DocFileZipStatusEnum.FINISHED.getCode());
                        finalKnowDocZipFileDO.setFinishTime(LocalDateTime.now());
                    }catch (Exception e) {
                        log.error("下载打包文件异常", e);
                        finalKnowDocZipFileDO.setStatus(DocFileZipStatusEnum.ERROR.getCode());
                        finalKnowDocZipFileDO.setFinishTime(LocalDateTime.now());
                    }finally {
                        knowDocZipFileDomainService.saveZipFile(finalKnowDocZipFileDO);
                    }
                });
            }

        } else {
            if (DocFileZipStatusEnum.DOWNLOADING.codeEquals(knowDocZipFileDO.getStatus())) {
                // 文档正在下载，或正在打包中，返回固定错误码
                throw new JrBizException(KnowBizErrorCode.DOC_FILE_DOWNLOADING_ERROR);
            }
            // 返回压缩包地址
            String zipFileId = knowDocZipFileDO.getZipFileId();
            String zipFileName = knowDocZipFileDO.getZipFileName();
            return Collections.singletonList(getDocFileDownloadToken(zipFileId, zipFileName));
        }
    } else {
        // 直接返回下载链接数组
        return docFileList.stream().filter(docFile -> StringUtils.isNotEmpty(docFile.getDocFileId()))
                .map(docFile -> getDocFileDownloadToken(docFile.getDocFileId(), docFile.getDocFileName())).collect(Collectors.toList());
    }
}


private AuthCheckDTO getDocFileDownloadToken(String docFileId, String originFileName) {
    FileAddTokenReq req = new FileAddTokenReq();
    req.setDocId(docFileId);
    req.setFileName(originFileName);
    req.setToken(IdUtil.simpleUUID());
    req.setUserCode(LoginContext.getLoginInfo().getMip());

    AssertUtil.isTrue(fileAuthFacade.submitAuthToken(req).isSuccess(), "获取权限失败", KnowBizErrorCode.DUBBO_REQ_ERROR.getInnerErrCode());

    AuthCheckDTO result = new AuthCheckDTO();
    result.setDocId(docFileId);
    result.setToken(req.getToken());
    return result;
}
private String zipDocFiles(String userId, String zipFileName, List<KnowDocFileDO> docFileList) {
    // todo 多线程下载
    File[] localFileList = docFileList.parallelStream()
            .filter(docFile -> StringUtils.isNotEmpty(docFile.getDocFileId()))
            .map(docFile -> downloadDocFileToLocal(docFile, userId))
            .toArray(File[]::new);

    // 压缩打包
    File zipFile = FileUtil.file(commonConfig.getTempLocalPath(), userId, zipFileName);
    try {
        ZipUtil.zip(zipFile, false, localFileList);
    } catch (Exception e) {
        log.error("压缩文件异常", e);
        throw new BizException("压缩文件异常", e);
    } finally {
        for (File file : localFileList) {
            FileUtils.deleteQuietly(file);
        }
    }

    try {
        long id = documentService.upload(zipFile, CommonConstant.BUSINESS_NO, userId);

        return String.valueOf(id);
    } catch (Exception e) {
        log.error("上传文档中心失败", e);
        throw new RuntimeException(e);
    } finally {
        FileUtils.deleteQuietly(zipFile);
    }
}

private File downloadDocFileToLocal(KnowDocFileDO fileDO, String userId) {
    FileGetUrlReq fileUrlReq = new FileGetUrlReq();
    fileUrlReq.setDocId(fileDO.getDocFileId());
    ApiResult<String> downloadResult = fileDownloadFacade.getFileUrl(fileUrlReq);
    AssertUtil.isTrue(downloadResult.isSuccess(), "文件下载失败: " + downloadResult.getMsg());

    File temp = FileUtil.file(commonConfig.getTempLocalPath(), userId,
            // 实际文件名使用docId进行拼接，因存在部分同名文件
            String.join("_", fileDO.getDocId(), fileDO.getDocFileName()));
    HttpUtil.downloadFile(downloadResult.getData(), temp);
    return temp;
}
```

# 领域知识库 爱玛环境客户端和管理端显示数据不一致 题解

Ld的排查小课堂

首先看log，看出来调取了common-search的方法去查es，有一个es的查询报错400，找到为什么报错，

log显示是有一个know_id显示在爱玛和poc环境下显示的是text类型，而dev正常类型下是keyword类型，

而由于es中text类型不能排序等操作，而keyword可以，所以得把这个字段换成keyword类型

而由于es的api不能直接修改字段而是要先创建一个新索引再迁移再删除旧索引，修改过后用户端就正常能查出来了

然而由于我之前傻逼 改了一个数据库数据 导致用户和管理端有一条数据只有用户端有。查了一会觉得是测试时候的脏数据于是删掉了，同时还要删es数据。

给我复盘压力我：

1. 排查日志的时候要一步一步来看 日志为准再去顺着看代码逻辑
2. spring和k8s里面配置装配原理不太懂所以没找到mgr里哪里调用了common-search去查es

# knowledge-mgr-admin核心逻辑

## 文档下载

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222043082.png)

## 导入标签

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222043592.png)

## 导入标签失败重试

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=NThkYTgwODc2MGY2Y2JhMjgyMzc2Y2VmYWNkNGI1YmNfTVlHVFQ3V2V3NVlEaE91cnF5bUtzek5PZ0IzeEhJSElfVG9rZW46SWJZRWJCWWl5b05RRzd4T292SGNYVjlobjZlXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

# t-common-search模块

KMP和词典树构造、基于AC自动机的字符串高亮、bitmap、RRF、RAG、一次二次召回、多路召回、N*M阵列召回*、*embedding*、*向量数据库、LiteFlow规则引擎

```Java
@startuml

actor "用户" as User
participant "KnowledgeBasePreProcessor" as KBPre
participant "QueryProcessor" as Query
participant "EmbeddingProcessor" as Embedding
participant "RecallStrategyProcessor" as RecallStrategy
participant "KnowledgeBaseRecallProcessor" as KBRecall
participant "KnowledgeForwardIndexProcessor" as ForwardIndex
participant "KnowledgeBaseRerankProcessor" as Rerank
participant "KnowledgeBaseWrapResultProcessor" as WrapResult
participant "HighlightUtils" as Highlight
database "数据库" as DB

User -> KBPre: 发起知识库搜索请求
activate KBPre
KBPre -> Query: 初始化查询参数
activate Query
Query -> DB: 查询词分析
DB --> Query: 返回查询词分析结果
Query --> KBPre: 设置查询词分析结果
deactivate Query
KBPre -> Embedding: 调用embedding服务
activate Embedding
Embedding -> DB: 获取embedding结果
DB --> Embedding: 返回embedding结果
Embedding --> KBPre: 设置embedding结果
deactivate Embedding
KBPre -> RecallStrategy: 传递预处理后的请求
deactivate KBPre

activate RecallStrategy
RecallStrategy -> KBRecall: 根据召回策略构造ES请求
activate KBRecall
KBRecall -> DB: 第一批次召回（文本全匹配、向量召回等）
DB --> KBRecall: 返回第一批召回结果
KBRecall -> DB: 第二批次召回（丢词、丢停用词等）
DB --> KBRecall: 返回第二批召回结果
KBRecall -> ForwardIndex: 传递召回结果
deactivate KBRecall

activate ForwardIndex
ForwardIndex -> DB: 正排索引查询
DB --> ForwardIndex: 返回正排索引查询结果
ForwardIndex -> Rerank: 传递正排查询结果
deactivate ForwardIndex

activate Rerank
Rerank -> DB: 调用重排序服务
DB --> Rerank: 返回重排序结果
Rerank -> WrapResult: 传递重排序结果
deactivate Rerank

activate WrapResult
WrapResult -> Highlight: 提取查询词分析结果中的关键词
activate Highlight
Highlight -> DB: 构建AC自动机
DB --> Highlight: 返回AC自动机构建结果
Highlight -> DB: 对文档内容进行AC自动机匹配并构建Bitmap
DB --> Highlight: 返回Bitmap匹配结果
Highlight -> WrapResult: 根据Bitmap对文档内容进行高亮处理
deactivate Highlight
WrapResult -> User: 返回最终搜索结果
deactivate WrapResult

note over KBPre:: KnowledgeBasePreProcessor\n预处理阶段，初始化查询参数，设置调试模式等。
note over Query:: QueryProcessor\n查询词分析，使用分词工具对查询词进行分词处理。
note over Embedding:: EmbeddingProcessor\n调用embedding服务，将查询词转换为向量表示，用于向量召回。
note over RecallStrategy:: RecallStrategyProcessor\n根据召回策略构造ES请求，决定使用哪种召回方式。
note over KBRecall:: KnowledgeBaseRecallProcessor\n执行多路召回，包括文本全匹配、向量召回等。\n第一批召回：文本全匹配、跨字段全匹配、向量召回。\n第二批召回：丢词、丢停用词等。
note over ForwardIndex:: KnowledgeForwardIndexProcessor\n正排索引查询，根据召回结果获取文档片段内容。
note over Rerank:: KnowledgeBaseRerankProcessor\n调用重排序服务，对召回结果进行重排序。\n使用RRF（Rank Fusion）融合不同召回结果。
note over WrapResult:: KnowledgeBaseWrapResultProcessor\n对最终结果进行封装，返回给用户。
note over Highlight:: HighlightUtils\n使用AC自动机对文档内容进行高亮处理。

@enduml
```

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222043531.png)

1. ## KMP（Knuth-Morris-Pratt）算法

逻辑：

- KMP算法是一种高效的字符串匹配算法，能够在O(n)的时间复杂度内完成模式匹配。
- 它通过预处理模式字符串，构建一个部分匹配表（也称为失配表），用于在匹配失败时快速跳过不需要比较的位置。

在知识库搜索中的作用：

- 在`KmpService`中调用KMP服务，用于快速检索知识库中的文档或知识点。
- 通过KMP算法，可以高效地在大规模文档集合中查找与查询词匹配的文档。

1. ## 词典树构造

逻辑：

- 词典树（Trie树）是一种用于快速检索字符串数据结构。
- 构造词典树时，将所有关键词插入到树中，每个节点表示一个字符，路径表示一个字符串。

在知识库搜索中的作用：

- 在`HighlightUtils`中使用词典树存储查询词的分词结果，用于快速匹配和高亮显示。
- 词典树能够高效地存储和检索关键词，支持快速的字符串匹配和高亮显示。

1. ## 基于AC自动机的字符串高亮

逻辑：

- AC自动机（Aho-Corasick自动机）是一种多模式匹配算法，能够在O(n)的时间复杂度内完成多个模式的匹配。
- 构建AC自动机时，将所有关键词插入到自动机中，自动机能够同时匹配多个关键词。

在知识库搜索中的作用：

- 在`HighlightUtils`中使用AC自动机进行字符串高亮处理。
- 通过AC自动机匹配关键词，并在匹配到的位置添加高亮标签，提高用户体验。

1. ## Bitmap

逻辑：

- Bitmap（位图）是一种高效的数据结构，用于存储和检索集合中的元素。
- 每个位表示一个元素的存在或不存在，适用于大规模数据的快速检索。

在知识库搜索中的作用：

- 在`HighlightUtils`中使用Bitmap存储匹配位置，用于快速构建高亮结果。
- Bitmap能够高效地存储和检索匹配位置，支持快速的字符串高亮显示。

1. ## RRF（Relevance Rank Fusion）

逻辑：

- RRF是一种融合不同召回结果的方法，能够提高召回的准确性和覆盖率。
- RRF通过加权融合不同召回结果，生成最终的排序结果。

在知识库搜索中的作用：

- 在`KnowledgeBaseRecallProcessor`和`FusionRRFProcessor`中实现RRF融合逻辑。
- 通过RRF融合不同召回结果，生成最终的排序结果，提高召回的准确性和覆盖率。

1. ## RAG（Retrieval-Augmented Generation）

逻辑：

- RAG是一种结合检索和生成的模型，能够从大规模文档中检索相关信息，并生成高质量的答案。
- RAG通过检索相关文档，提取关键信息，并生成自然语言答案。

在知识库搜索中的作用：

- 在`KnowledgeBaseRerankProcessor`中可能使用RAG模型，对召回结果进行重排序和生成高质量的答案。
- RAG能够结合检索和生成，提高答案的质量和相关性。

1. ## 一次二次召回

逻辑：

- 一次召回（First Batch Recall）：使用主要的召回策略，如文本全匹配、向量召回等。
- 二次召回（Second Batch Recall）：在一次召回结果不足时，使用补充策略，如丢词、丢停用词等。

在知识库搜索中的作用：

- 在`KnowledgeBaseRecallProcessor`中实现一次和二次召回。
- 一次召回确保主要结果的覆盖，二次召回补充不足的结果，提高召回的全面性。

### 补充：可以写java粗筛、py精筛

将召回过程拆分为多个批次（Batch）逐步执行，逐步缩小候选集范围，平衡效果与性能。

**典型流程**

1. **粗筛（Coarse Recall）**：用简单规则或模型（如热门商品、用户近期行为）快速过滤出大候选集（如10万条）。
2. **精筛（Fine Recall）**：对粗筛结果用复杂模型（如深度模型）进一步筛选，得到小候选集（如1000条）。

**优点**

- 降低计算压力，避免直接对全量数据跑复杂模型。
- 可结合实时/离线数据分层处理。

**应用场景**

- 电商推荐：先召回用户品类偏好商品，再筛选高评分商品。
- 广告系统：先按广告主预算粗筛，再按CTR模型精筛。

1. ## 多路召回

逻辑：

- 多路召回（Multi-Recall）：同时使用多种召回策略，如文本全匹配、向量召回、丢词召回等。
- 每种召回策略生成不同的结果，最终融合生成最终召回结果。

在知识库搜索中的作用：

- 在`KnowledgeBaseRecallProcessor`中实现多路召回。
- 多路召回能够从多个角度获取结果，提高召回的全面性和准确性。

1. ## N*M阵列召回

逻辑：

- N*M阵列召回（N*M Array Recall）：使用N个召回策略，每个策略生成M个结果，形成N*M的阵列。
- 通过阵列召回，能够覆盖更多的结果，提高召回的全面性。

在知识库搜索中的作用：

- 在`KnowledgeBaseRecallProcessor`中可能实现N*M阵列召回。
- N*M阵列召回能够覆盖更多的结果，提高召回的全面性和准确性。

1. ## Embedding

逻辑：

- Embedding是一种将文本转换为高维向量的技术，用于表示文本的语义信息。
- 通过训练模型，将文本转换为向量表示，便于进行向量召回和相似性计算。

在知识库搜索中的作用：

- 在`EmbeddingProcessor`中调用embedding服务，将查询词转换为向量表示。
- 使用embedding结果进行向量召回，提高召回的准确性和相关性。

这段代码使用了 Reactor 框架（一个响应式编程库）来并行处理一个任务列表，并最终收集结果。它的主要目的是对 `embeddingReqList` 中的每个 `EmbeddingReq` 对象进行异步处理，并将处理结果收集到一个 `List<EmbeddingResp>` 中。以下是逐步解析：

```Java
List<EmbeddingResp> resultList = Flux.fromIterable(embeddingReqList)
       .parallel()
       .runOn(embeddingScheduler)
       .map(each -> SupplierWrapper.of(() -> embeddingService.embedSentences(each.getSentences(), each.getLanguage())))
       .map(SupplierWrapper::get)
       .sequential()
       .collectList()
       .block();
```

1. ## 向量数据库

逻辑：

- 向量数据库是一种存储和检索高维向量的数据库，支持快速的相似性搜索。
- 通过向量数据库，能够高效地存储和检索向量表示的文本。

在知识库搜索中的作用：

- 在`KnowledgeBaseRecallProcessor`中使用向量数据库进行向量召回。
- 向量数据库能够高效地存储和检索向量表示的文本，支持快速的相似性搜索。

# ld整理的 t-common-search 文档：

https://git.midea.com/DEP-MEICLOUD/PROD/LLM/STD/knowledge/t-common-searcher/-/blob/fix/analyze/docs/docs.md

## 技术文档

## 开始

本文档用于帮助用户快速了解项目的技术设计，主要包含以下内容：核心功能、系统架构、关键实现、数据库设计等

## 核心功能

- 基于目录搜索

```Plain
/t-common-search/knowledge/catalogSearch
```

- 知识搜索

```Plain
/t-common-search/knowledge/knowledgeSearch
```

## 系统架构

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=MWE4OWI2YzcxZGJhN2Q2NmZmODk4ZTQ1MzI1YzMwYzhfTTRnd3lpMFhGUGp4MlNOMXRobmFYZFVUVVE4RTFmOHpfVG9rZW46UE9MeGI2eWt2bzl5UWl4WURPeWN3ZWI2bmNkXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

- 中间件 
  - ES：关键字搜索；充当向量数据词，向量搜索
- 依赖服务 
  - Embedding：向量化服务
  - Kmp: 集团知识搜索服务，可删除
  - Knowledge：知识库服务，即knowledge-mgr，用于判断权限
  - Query：分词服务；目前均为调用失败的，走本地分词
  - Rerank：重排服务；目前未部署，调用均失败，exception被catch忽略
- Liteflow框架 
  - 开源架构，可参考：https://liteflow.cc/

## 关键实现

使用场景1： 

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=MjJkMDIxZWU0MmJjM2ViYTVmOTcxZmQwZDQwMjY1YTRfeVZ2VTlyT3NUQXpKZm9lckYxekVIbzlKTUtsM2hoV1FfVG9rZW46QXBaUGJSV2hOb25kenJ4N1RPTGNJc1BobmplXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

调用流程如下： 

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222043770.png)

 其中commPre、query、recallStrategy、recall、commWrapResult分别为liteflow流程中的自定义节点，继承NodeComponent，实现process方法。

使用场景2： 

![img](https://wxkhfetflyx.feishu.cn/space/api/box/stream/download/asynccode/?code=YTdmNTQyNWYwYzAxZGI5MjczN2FhNjI4NzE0OTdkYmZfa05IVllIMjFNeDQ5aTM1enVpS21XQVlOSHhnTnVuUXdfVG9rZW46U0szcWJLb0V5b1pqTGZ4aHRKcGM5bzAxbklnXzE3NDc5MTc3NzE6MTc0NzkyMTM3MV9WNA)

调用流程如下： 

![img](https://raw.githubusercontent.com/raosirui/Picture/main/markdown/202505222043542.png)

- 召回阶段：主要目标是快速从大规模数据中找到可能相关的文档集合，此时更注重响应速度和召回率（即覆盖面），可能会牺牲一些精确性。
- 正排阶段：在召回的基础上，通过正排查询获取每个候选文档的详细信息，包括文档内容、元数据等，为后续的精确排序提供数据支持。
- 重排阶段：基于更复杂的算法（如机器学习模型），对正排结果进行重新排序，以提高结果的相关性和精确性。

## 配置项

| key                           | value                                                        | 说明                           |
| ----------------------------- | ------------------------------------------------------------ | ------------------------------ |
| embedding.service.mideaEnable | FALSE                                                        | 是否使用美的向量化模型         |
| embedding.service.enUrl       | http://ai-models-dev.meicloud.com/m3e-base/v2/embedding      | 向量化服务地址（英文）         |
| embedding.service.url         | http://ai-models-dev.meicloud.com/bge-m3/v2/embedding        | 向量化服务                     |
| embedding.service.m3eBaseUrl  | http://ai-models-dev.meicloud.com/m3e-base/v2/embedding      | 没用的，可删                   |
| knowledge.index               | knowledge-key-words-search-index-${aigc.env}                 | 知识库索引                     |
| knowledge.service.secret      | xx                                                           | 调用knowledge-mgr的秘钥        |
| knowledge.service.sourceCode  | knowledge                                                    | 来源系统                       |
| knowledge.service.url         | https://aigc-dev.meicloud.com                                | knowledge-mgr的域名            |
| midea.course.cache.switch     |                                                              | 没用的                         |
| midea.course.clear.segcache   |                                                              | 没用的                         |
| midea.course.experiment.user  |                                                              | 没用的                         |
| midea.course.filter.employee  |                                                              | 没用的                         |
| midea.course.index            |                                                              | 没用的                         |
| query.http.switch             | 1                                                            | 只能是1，没必要                |
| query.seg.op                  |                                                              | 没用的                         |
| rerank.service.url            | https://apiprod.midea.com/T-Search/t-search-semantic-reranker/v1/semanticReranker | 目前没服务？                   |
| rerank.switch                 | 1                                                            | 是否重排                       |
| rerank.topKTimes              | 3                                                            | topK的倍数                     |
| query.service.url             | http://t-search-query.nh-prd-1.cce.com/v1/query              | 没用的                         |
| rough.rank.http.url           | http://search-prod.prod.cce.com/api/ihr/prerank              | 没用的                         |
| rough.rerank.switch           | 1                                                            |                                |
| total.switch                  | 1                                                            | 是否开启es的trackTotalHits功能 |
| train.course.index            | ihr_train_course                                             | 没用的                         |
| index.environment             | dev                                                          | 多余                           |
| spring.datasource             |                                                              | 没用的                         |
| spring.kafka                  |                                                              | 没用的                         |

## 可优化项

参见代码中的TODO

## 问题

1. rerank服务
2. 召回的逻辑
3. 正排逻辑